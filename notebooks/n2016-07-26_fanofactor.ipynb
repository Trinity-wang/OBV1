{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSAConnector: libneurosim support not available in NEST.\n",
      "Falling back on PyNN's default CSAConnector.\n",
      "Please re-compile NEST using --with-libneurosim=PATH\n"
     ]
    }
   ],
   "source": [
    "from RRNN import RRNN\n",
    "from NeuroTools.signals.spikes import SpikeTrain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = RRNN(recurrent=True, ring=False)\n",
    "df, spikesE, spikesI = net.model()\n",
    "\n",
    "for spiketrain in spikesE.spiketrains:\n",
    "    print 'Fano factor : {0}'.format(SpikeTrain(spiketrain).fano_factor_isi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fano factor forces us to manage case of silent neurons. Another possible approach is to compute an equivalent index using cv of collapsed activity of population neurons over cvs mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 6], [1, 3, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (A.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (np.unique(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.unique() is apparently the function to use with our spiketrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab = np.array(spikesE.spiketrains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tab = np.array([])\n",
    "for st in spikesE.spiketrains:\n",
    "    tab = np.append(tab, np.array(st), axis=0)\n",
    "print tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab2 = np.unique(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (tab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_collapsedspikes = SpikeTrain(tab2).cv_isi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cv_collapsedspikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_CVs = np.array([])\n",
    "for st in spikesE.spiketrains :\n",
    "    all_CVs = np.append(all_CVs, SpikeTrain(np.array(st)).cv_isi())\n",
    "meanCv = np.nanmean(all_CVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(meanCv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakefano = cv_collapsedspikes/meanCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fakefano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a little test with poisson process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyNN.nest as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08037316925\n"
     ]
    }
   ],
   "source": [
    "sim.setup(timestep=0.1)\n",
    "\n",
    "poisson = sim.Population(100, sim.SpikeSourcePoisson(rate=10))\n",
    "poisson.record('spikes')\n",
    "\n",
    "sim.run(1000)\n",
    "poisson_spikes = poisson.get_data().segments[0]\n",
    "\n",
    "tab = np.array([])\n",
    "for st in poisson_spikes.spiketrains:\n",
    "    tab = np.append(tab, np.array(st), axis=0)\n",
    "\n",
    "cv_collapsed = SpikeTrain(np.unique(tab)).cv_isi()\n",
    "\n",
    "all_CVs = np.array([])\n",
    "for st in poisson_spikes.spiketrains :\n",
    "    all_CVs = np.append(all_CVs, SpikeTrain(np.array(st)).cv_isi())\n",
    "\n",
    "meanCv = np.nanmean(all_CVs)\n",
    "\n",
    "fakefano = cv_collapsed/meanCv\n",
    "\n",
    "print fakefano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  60.6246742 ,  161.58848008,  167.32803998,  224.11030335,\n",
       "        577.45015467,  974.23254283])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(poisson_spikes.spiketrains[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspike\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "cd ~/Downloads\n",
    "git clone https://github.com/mariomulansky/PySpike.git\n",
    "cd PySpike\n",
    "python setup.py build_ext --inplace\n",
    "python setup.py build\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliability = 4.2274\n"
     ]
    }
   ],
   "source": [
    "def reliability(spike_data, method='purpura'):\n",
    "    spike_trains = []    \n",
    "    for st_ in spike_data:\n",
    "        spike_trains.append(st_)\n",
    "\n",
    "    if method=='pyspike':\n",
    "        import pyspike as spk\n",
    "        spike_trains = [ spk.SpikeTrain(st_, edges=(0, params['block_duration'])) for st_ in spike_trains]   # spike_trains.append(spk.SpikeTrain(st_, edges=(0, params['block_duration'])))\n",
    "\n",
    "        #from pyspike import spike_sync_profile as profile # , max_tau=1000\n",
    "        from pyspike import spike_profile as profile\n",
    "        spk.disable_backend_warning = True\n",
    "        avrg_profile = profile(spike_trains).avrg()\n",
    "\n",
    "    elif method=='purpura':\n",
    "        import elephant\n",
    "        from neo.core import SpikeTrain\n",
    "        from quantities import s\n",
    "\n",
    "        #spike_trains = [ SpikeTrain(st_*s, t_stop=1*s) for st_ in spike_trains] \n",
    "        #for st_ in spike_data[:8]: # HACK to remove the last trials which seem weak\n",
    "        #    spike_trains.append(SpikeTrain(st_*s, t_stop=params['block_duration']*s))\n",
    "        D = elephant.spike_train_dissimilarity.victor_purpura_dist(spike_trains)\n",
    "        avrg_profile = D.mean()\n",
    "\n",
    "    elif method=='van_rossum':\n",
    "        import elephant\n",
    "        from neo.core import SpikeTrain\n",
    "        from quantities import s\n",
    "\n",
    "        spike_trains = [ SpikeTrain(st_*s, t_stop=params['block_duration']*s) for st_ in spike_trains] \n",
    "        #for st_ in spike_data[:8]: # HACK to remove the last trials which seem weak\n",
    "        #    spike_trains.append(SpikeTrain(st_*s, t_stop=params['block_duration']*s))\n",
    "        D = elephant.spike_train_dissimilarity.van_rossum_dist(spike_trains, tau=np.array(1.0) * s)\n",
    "        avrg_profile = D.mean()\n",
    "\n",
    "    return avrg_profile\n",
    "            \n",
    "\n",
    "print (\"reliability = \" + \"{0:.4f}\".format(reliability(poisson_spikes.spiketrains)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = RRNN(recurrent=True, ring=False)\n",
    "df, spikesE, spikesI = net.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliability = 4.0763\n"
     ]
    }
   ],
   "source": [
    "print (\"reliability = \" + \"{0:.4f}\".format(reliability(spikesE.spiketrains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
